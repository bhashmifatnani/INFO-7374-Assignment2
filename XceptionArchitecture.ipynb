{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XceptionArchitecture.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qNOUzi8W403v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Xception Architecture"
      ]
    },
    {
      "metadata": {
        "id": "vC0eTgZe403z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Importing libraries and code for loading resized images "
      ]
    },
    {
      "metadata": {
        "id": "M7sfYmhg4032",
        "colab_type": "code",
        "colab": {},
        "outputId": "e3066e92-674b-4db9-dfa5-5e2c32a01c70"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from keras.datasets import cifar10\n",
        "import cv2\n",
        "import keras as k\n",
        "from keras import models\n",
        "from matplotlib import pyplot as p\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential,Model\n",
        "from keras import optimizers\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten,BatchNormalization, Input,Lambda\n",
        "from keras.layers import Conv2D, MaxPooling2D,GlobalAveragePooling2D\n",
        "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications import VGG16\n",
        "from keras.layers import LeakyReLU\n",
        "import pandas as pd\n",
        "import os \n",
        "from PIL import Image\n",
        "\n",
        "def load_images(path,num_classes):\n",
        "    #Load images\n",
        "    \n",
        "    print('Loading ' + str(num_classes) + ' classes')\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    #X_train=np.zeros([num_classes*500,227,227,3],dtype='uint8')\n",
        "    #y_train=np.zeros([num_classes*500], dtype='uint8')\n",
        "\n",
        "    trainPath=path+'/train/train/bilinear'\n",
        "\n",
        "    print('loading training images...');\n",
        "\n",
        "    i=0\n",
        "    j=0\n",
        "    annotations={}\n",
        "    for sChild in os.listdir(trainPath):\n",
        "        sChildPath = os.path.join(os.path.join(trainPath,sChild),'images')\n",
        "        annotations[sChild]=j\n",
        "        for c in os.listdir(sChildPath):\n",
        "            spath=os.path.join(sChildPath,c)   \n",
        "            img = cv2.imread(spath)\n",
        "            img = cv2.resize(img, (139, 139))\n",
        "            X_train.append(img)\n",
        "            y_train.append(j)\n",
        "            i+=1\n",
        "        j+=1\n",
        "        if (j >= num_classes):\n",
        "            break\n",
        "\n",
        "    print('finished loading training images')\n",
        "\n",
        "    val_annotations_map = get_annotations_map()\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "    print('loading test images...')\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "    i = 0\n",
        "    testPath=path +'/val/bilinear/images'\n",
        "    print(testPath + \"abc\")\n",
        "    for sChild in os.listdir(testPath):\n",
        "        if val_annotations_map[sChild] in annotations.keys():\n",
        "            sChildPath = os.path.join(testPath, sChild)\n",
        "            img = cv2.imread(sChildPath)\n",
        "            img = cv2.resize(img, (139, 139))\n",
        "            X_test.append(img)\n",
        "            y_test.append(annotations[val_annotations_map[sChild]])\n",
        "            i+=1\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "\n",
        "    print('finished loading test images'+str(i))\n",
        "    np.save('data/saved_npy/tf/y_train.npy',y_train)\n",
        "    np.save('data/saved_npy/tf/x_train.npy',X_train)\n",
        "    np.save('data/saved_npy/tf/x_test.npy',X_test)\n",
        "    np.save('data/saved_npy/tf/y_test.npy',y_test)\n",
        "    return X_train,y_train,X_test,y_test\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "weJ3EnGa403_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_annotations_map():\n",
        "\tvalAnnotationsPath = 'C:/Users/bhash/Downloads/val_annotations.txt'\n",
        "\tvalAnnotationsFile = open(valAnnotationsPath, 'r')\n",
        "\tvalAnnotationsContents = valAnnotationsFile.read()\n",
        "\tvalAnnotations = {}\n",
        "\n",
        "\tfor line in valAnnotationsContents.splitlines():\n",
        "\t\tpieces = line.strip().split()\n",
        "\t\tvalAnnotations[pieces[0]] = pieces[1]\n",
        "\n",
        "\treturn valAnnotations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0fuao9R_404F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading Images"
      ]
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "9udY3ecS404H",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1bc78ac-30f1-4692-a0c2-e319cbf08baf"
      },
      "cell_type": "code",
      "source": [
        "path='C:/Users/bhash/data'\n",
        "x_train,y_train,x_test,y_test=load_images(path,200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading 200 classes\n",
            "loading training images...\n",
            "finished loading training images\n",
            "loading test images...\n",
            "C:/Users/bhash/data/val/bilinear/imagesabc\n",
            "finished loading test images10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q5VXYXUJ404N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "def xception_from_disk(path = 'data/my_xception.h5'):\n",
        "    if glob(path):\n",
        "        fc_model = load_model(path)\n",
        "        print('Xception Model Loaded from Disk!')\n",
        "    else:\n",
        "        fc_model = fc_layer(features_train)\n",
        "        fc_model.save(path)\n",
        "    return fc_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HYxAbR7d404T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "def load_traindata():\n",
        "    if glob('data/saved_npy'):\n",
        "        y_train = np.load('data/saved_npy/tf/y_train.npy')\n",
        "        x_train = np.load('data/saved_npy/tf/x_train.npy')\n",
        "        x_test = np.load('data/saved_npy/tf/x_test.npy')\n",
        "        y_test = np.load('data/saved_npy/tf/y_test.npy')\n",
        "        print('Training data loaded from disk!')\n",
        "    else: \n",
        "        print('No saved data found. Creating..')\n",
        "        x_train, y_train = create_traindata()\n",
        "    return x_train, y_train, x_test, y_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_Zwrikt404Y",
        "colab_type": "code",
        "colab": {},
        "outputId": "b767b4de-7509-4e1a-c7d6-929596e68481"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "x_train, y_train,x_test,y_test = load_traindata()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data loaded from disk!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6_V_CZGU404g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Preproessing data"
      ]
    },
    {
      "metadata": {
        "id": "ei3sqYPQ404h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_train = k.utils.to_categorical(y_train, 200)\n",
        "y_test = k.utils.to_categorical(y_test, 200)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vb1MJd0F404m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train = features_train.astype('float32')\n",
        "x_test = features_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1jxULhm5404r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Methods to create  model and save the features of our model "
      ]
    },
    {
      "metadata": {
        "id": "2xnJpXlw404s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cnn_load_model(model='xception'):\n",
        "    im_size=139\n",
        "    if model == 'xception':\n",
        "        cnn_model = Xception(include_top=True, input_shape=(im_size, im_size, 3), weights='imagenet')\n",
        "    else:\n",
        "        cnn_model = InceptionV3(include_top=True, input_shape=(im_size, im_size, 3), weights='imagenet')\n",
        "    \n",
        "    for layer in cnn_model.layers:\n",
        "        layer.trainable = False\n",
        "    \n",
        "    inputs = Input((im_size, im_size, 3))\n",
        "    x = inputs\n",
        "   # x = Lambda(preprocess_input, name='preprocessing')(x)\n",
        "    x = cnn_model(x)\n",
        "    #x = GlobalAveragePooling2D()(x)\n",
        "    #x = GlobalAveragePooling2D()(x)\n",
        "    cnn_model = Model(inputs, x)\n",
        "    cnn_model.summary()\n",
        "    return cnn_model\n",
        "\n",
        "def bottleneck_features(model='xception'):\n",
        "    print('Loading model:',model)\n",
        "    if glob('data/saved_npy/xception_features_train.npy'):\n",
        "        print(\"op\")\n",
        "        features_train = np.load('data/saved_npy/xception_features_train.npy')\n",
        "        features_test = np.load('data/saved_npy/xception_features_test.npy')\n",
        "        print('Features loaded from disk!')\n",
        "    else:\n",
        "        print(\"hi\")\n",
        "        xception_m = cnn_load_model(model)\n",
        "        features_train = xception_m.predict(x_train,batch_size=64, verbose=1)\n",
        "        features_test = xception_m.predict(x_test,batch_size=64, verbose=1)\n",
        "        np.save('data/saved_npy/tf/xception_features_train.npy',features_train)\n",
        "        np.save('data/saved_npy/tf/xceptio_features_test.npy',features_test)\n",
        "    return features_train, features_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ql5RN2Ug404v",
        "colab_type": "code",
        "colab": {},
        "outputId": "e0097e51-0e2c-4dee-eab4-277b2ff92d67"
      },
      "cell_type": "code",
      "source": [
        "features_train, features_test = np.asarray(bottleneck_features(model='xception'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model: xception\n",
            "hi\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_8 (InputLayer)         (None, 139, 139, 3)       0         \n",
            "_________________________________________________________________\n",
            "xception (Model)             (None, 1000)              22910480  \n",
            "=================================================================\n",
            "Total params: 22,910,480\n",
            "Trainable params: 0\n",
            "Non-trainable params: 22,910,480\n",
            "_________________________________________________________________\n",
            "100000/100000 [==============================] - 11088s \n",
            "10000/10000 [==============================] - 956s   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_uFvjz-B404z",
        "colab_type": "code",
        "colab": {},
        "outputId": "b20eda87-1b26-431f-bbb3-d5eebdafbd44"
      },
      "cell_type": "code",
      "source": [
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 200)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "metadata": {
        "id": "bw3CcLNX4046",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  fitting model on features saved in local for xpection model"
      ]
    },
    {
      "metadata": {
        "id": "jwlX5u-f4047",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ]
    },
    {
      "metadata": {
        "id": "C6ziyCI-4049",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fc_layer(features=features_train,y_train = y_train):\n",
        "#     if y_train is None:\n",
        "#         y_train = np.load('data/saved_npy/y_train.npy')\n",
        "    \n",
        "    num_class = 200\n",
        "    inputs = Input(features.shape[1:])\n",
        "    x = inputs\n",
        "    x = Dense(4096,activation = None)(x)\n",
        "    x = Dense(512,activation = None)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = Dense(num_class, activation='softmax')(x)\n",
        "    model = Model(inputs, x)\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    h = model.fit(features_train, y_train, batch_size=64, validation_data=(x_test,y_test),epochs = 5)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QK6T6TLi405C",
        "colab_type": "code",
        "colab": {},
        "outputId": "9daa4bf6-e4b2-42bf-dad0-f6b67b6a5589"
      },
      "cell_type": "code",
      "source": [
        "fc_model2 = xception_from_disk()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "100000/100000 [==============================] - 165s - loss: 5.2698 - acc: 0.0083 - val_loss: 5.2987 - val_acc: 0.0050\n",
            "Epoch 2/5\n",
            "100000/100000 [==============================] - 177s - loss: 5.2547 - acc: 0.0091 - val_loss: 5.2993 - val_acc: 0.0050\n",
            "Epoch 3/5\n",
            "100000/100000 [==============================] - 184s - loss: 5.2485 - acc: 0.0093 - val_loss: 5.2991 - val_acc: 0.0050\n",
            "Epoch 4/5\n",
            "100000/100000 [==============================] - 187s - loss: 5.2447 - acc: 0.0096 - val_loss: 5.2988 - val_acc: 0.0050\n",
            "Epoch 5/5\n",
            "100000/100000 [==============================] - 184s - loss: 5.2417 - acc: 0.0095 - val_loss: 5.3014 - val_acc: 0.0050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Hm4tAIWd405I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 2"
      ]
    },
    {
      "metadata": {
        "id": "I474pXXr405K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fc_layer(features=features_train,y_train = y_train):\n",
        "#     if y_train is None:\n",
        "#         y_train = np.load('data/saved_npy/y_train.npy')\n",
        "    \n",
        "    num_class = 200\n",
        "    inputs = Input(features.shape[1:])\n",
        "    x = inputs\n",
        "    \n",
        "    x = Dense(512,activation = None)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_class, activation='softmax')(x)\n",
        "    model = Model(inputs, x)\n",
        "    #opt = optimizers.Adam(beta_1=.9,beta_2=.999,lr = 0.0001)\n",
        "    opt = optimizers.SGD(lr=.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    h = model.fit(features_train, y_train, batch_size=32, validation_data=(x_test,y_test),epochs = 5)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fwwcOeag405N",
        "colab_type": "code",
        "colab": {},
        "outputId": "f8772f17-8dbd-4838-a47c-edd11b34abec"
      },
      "cell_type": "code",
      "source": [
        "fc_model2 = xception_from_disk()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "100000/100000 [==============================] - 39s - loss: 5.2990 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0077\n",
            "Epoch 2/5\n",
            "100000/100000 [==============================] - 38s - loss: 5.2990 - acc: 0.0046 - val_loss: 5.2983 - val_acc: 0.0072\n",
            "Epoch 3/5\n",
            "100000/100000 [==============================] - 37s - loss: 5.2989 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0052\n",
            "Epoch 4/5\n",
            "100000/100000 [==============================] - 37s - loss: 5.2988 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 5/5\n",
            "100000/100000 [==============================] - 40s - loss: 5.2989 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3RIGiSWO405R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 3"
      ]
    },
    {
      "metadata": {
        "id": "_7frLJcw405S",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fc_layer(features=features_train,y_train = y_train):\n",
        "#     if y_train is None:\n",
        "#         y_train = np.load('data/saved_npy/y_train.npy')\n",
        "    \n",
        "    num_class = 200\n",
        "    inputs = Input(features.shape[1:])\n",
        "    x = inputs\n",
        "    \n",
        "    x = Dense(512,activation = None)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_class, activation='softmax')(x)\n",
        "    model = Model(inputs, x)\n",
        "    #opt = optimizers.Adam(beta_1=.9,beta_2=.999,lr = 0.0001)\n",
        "    opt = optimizers.SGD(lr=.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    h = model.fit(features_train, y_train, batch_size=64, validation_data=(x_test,y_test),epochs = 10)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IDZTUNqB405Z",
        "colab_type": "code",
        "colab": {},
        "outputId": "8b530627-1ea6-4073-8ea0-8cc4ffd77895"
      },
      "cell_type": "code",
      "source": [
        "fc_model2 = xception_from_disk()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 20s - loss: 5.2989 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0056\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 20s - loss: 5.2989 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0052\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 20s - loss: 5.2988 - acc: 0.0043 - val_loss: 5.2983 - val_acc: 0.0070\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 20s - loss: 5.2989 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0067\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 20s - loss: 5.2988 - acc: 0.0041 - val_loss: 5.2983 - val_acc: 0.0068\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 21s - loss: 5.2989 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0068\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 21s - loss: 5.2989 - acc: 0.0045 - val_loss: 5.2983 - val_acc: 0.0066\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 21s - loss: 5.2987 - acc: 0.0044 - val_loss: 5.2983 - val_acc: 0.0066\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 20s - loss: 5.2987 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0066\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 21s - loss: 5.2987 - acc: 0.0047 - val_loss: 5.2983 - val_acc: 0.0064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "kBmXgI8w405g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 4"
      ]
    },
    {
      "metadata": {
        "id": "9vRoroAo405h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fc_layer(features=features_train,y_train = y_train):\n",
        "#     if y_train is None:\n",
        "#         y_train = np.load('data/saved_npy/y_train.npy')\n",
        "    \n",
        "    num_class = 200\n",
        "    inputs = Input(features.shape[1:])\n",
        "    x = inputs\n",
        "    \n",
        "    x = Dense(256,activation = None)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_class, activation='softmax')(x)\n",
        "    model = Model(inputs, x)\n",
        "    #opt = optimizers.Adam(beta_1=.9,beta_2=.999,lr = 0.0001)\n",
        "    opt = optimizers.SGD(lr=.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    h = model.fit(features_train, y_train, batch_size=64, validation_data=(x_test,y_test),epochs = 10)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4mMPh1hX405j",
        "colab_type": "code",
        "colab": {},
        "outputId": "ac7415c6-30d4-45b4-ae61-556f36b06f5b"
      },
      "cell_type": "code",
      "source": [
        "fc_model2 = xception_from_disk()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 16s - loss: 5.2990 - acc: 0.0050 - val_loss: 5.2983 - val_acc: 0.0076\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2988 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0060\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2988 - acc: 0.0051 - val_loss: 5.2983 - val_acc: 0.0065\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2988 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0063\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2987 - acc: 0.0054 - val_loss: 5.2983 - val_acc: 0.0060\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2989 - acc: 0.0054 - val_loss: 5.2983 - val_acc: 0.0061\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2988 - acc: 0.0053 - val_loss: 5.2983 - val_acc: 0.0061\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2986 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0061\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2988 - acc: 0.0054 - val_loss: 5.2983 - val_acc: 0.0061\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 15s - loss: 5.2987 - acc: 0.0053 - val_loss: 5.2983 - val_acc: 0.0061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9PVLXSnx405o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 5"
      ]
    },
    {
      "metadata": {
        "id": "8BVOmVE7405p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fc_layer(features=features_train,y_train = y_train):\n",
        "#     if y_train is None:\n",
        "#         y_train = np.load('data/saved_npy/y_train.npy')\n",
        "    \n",
        "    num_class = 200\n",
        "    inputs = Input(features.shape[1:])\n",
        "    x = inputs\n",
        "    \n",
        "    x = Dense(128,activation = None)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_class, activation='softmax')(x)\n",
        "    model = Model(inputs, x)\n",
        "    #opt = optimizers.Adam(beta_1=.9,beta_2=.999,lr = 0.0001)\n",
        "    opt = optimizers.SGD(lr=.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    h = model.fit(features_train, y_train, batch_size=64, validation_data=(x_test,y_test),epochs = 20)\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uB7MuIV6405u",
        "colab_type": "code",
        "colab": {},
        "outputId": "df8a8771-2cb5-406c-d1ed-9061ba6430d3"
      },
      "cell_type": "code",
      "source": [
        "fc_model2 = xception_from_disk()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "100000/100000 [==============================] - 12s - loss: 5.2987 - acc: 0.0050 - val_loss: 5.2983 - val_acc: 0.0047\n",
            "Epoch 2/20\n",
            "100000/100000 [==============================] - 12s - loss: 5.2988 - acc: 0.0055 - val_loss: 5.2983 - val_acc: 0.0051\n",
            "Epoch 3/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2988 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0063\n",
            "Epoch 4/20\n",
            "100000/100000 [==============================] - 12s - loss: 5.2987 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0054\n",
            "Epoch 5/20\n",
            "100000/100000 [==============================] - 12s - loss: 5.2988 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 6/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2987 - acc: 0.0049 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 7/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2988 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0051\n",
            "Epoch 8/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2986 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 9/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2986 - acc: 0.0051 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 10/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2987 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 11/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2987 - acc: 0.0048 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 12/20\n",
            "100000/100000 [==============================] - 14s - loss: 5.2986 - acc: 0.0053 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 13/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2986 - acc: 0.0050 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 14/20\n",
            "100000/100000 [==============================] - 14s - loss: 5.2986 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0049\n",
            "Epoch 15/20\n",
            "100000/100000 [==============================] - 14s - loss: 5.2986 - acc: 0.0054 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 16/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2986 - acc: 0.0053 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 17/20\n",
            "100000/100000 [==============================] - 13s - loss: 5.2986 - acc: 0.0050 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 18/20\n",
            "100000/100000 [==============================] - 14s - loss: 5.2986 - acc: 0.0053 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 19/20\n",
            "100000/100000 [==============================] - 14s - loss: 5.2986 - acc: 0.0052 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 20/20\n",
            "100000/100000 [==============================] - 14s - loss: 5.2985 - acc: 0.0053 - val_loss: 5.2983 - val_acc: 0.0050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cOYqC2n1405x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Experiment 6"
      ]
    },
    {
      "metadata": {
        "id": "0yhTWMBj4050",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def fc_layer(features=features_train,y_train = y_train):\n",
        "#     if y_train is None:\n",
        "#         y_train = np.load('data/saved_npy/y_train.npy')\n",
        "    \n",
        "    num_class = 200\n",
        "    inputs = Input(features.shape[1:])\n",
        "    x = inputs\n",
        "    \n",
        "    #x = Dense(256,activation = None)(x)\n",
        "    #x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    x = Dense(num_class, activation='softmax')(x)\n",
        "    model = Model(inputs, x)\n",
        "    opt = optimizers.Adam(beta_1=.9,beta_2=.999,lr = 0.0001)\n",
        "    #opt =  optimizers.Adagrad(lr = 0.0001)\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    h = model.fit(features_train, y_train, batch_size=32, validation_data=(x_test,y_test),epochs = 10)\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rAy5bTrK4053",
        "colab_type": "code",
        "colab": {},
        "outputId": "8dbb1f2b-b04d-4912-eb2a-3c6ab5f7b296"
      },
      "cell_type": "code",
      "source": [
        "fc_model2 = xception_from_disk()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "100000/100000 [==============================] - 25s - loss: 5.2971 - acc: 0.0057 - val_loss: 5.2983 - val_acc: 0.0054\n",
            "Epoch 2/10\n",
            "100000/100000 [==============================] - 25s - loss: 5.2914 - acc: 0.0069 - val_loss: 5.2983 - val_acc: 0.0052\n",
            "Epoch 3/10\n",
            "100000/100000 [==============================] - 26s - loss: 5.2869 - acc: 0.0072 - val_loss: 5.2983 - val_acc: 0.0052\n",
            "Epoch 4/10\n",
            "100000/100000 [==============================] - 28s - loss: 5.2830 - acc: 0.0078 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 5/10\n",
            "100000/100000 [==============================] - 28s - loss: 5.2796 - acc: 0.0084 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 6/10\n",
            "100000/100000 [==============================] - 28s - loss: 5.2767 - acc: 0.0082 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 7/10\n",
            "100000/100000 [==============================] - 28s - loss: 5.2739 - acc: 0.0085 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 8/10\n",
            "100000/100000 [==============================] - 28s - loss: 5.2720 - acc: 0.0086 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 9/10\n",
            "100000/100000 [==============================] - 26s - loss: 5.2693 - acc: 0.0088 - val_loss: 5.2983 - val_acc: 0.0050\n",
            "Epoch 10/10\n",
            "100000/100000 [==============================] - 25s - loss: 5.2678 - acc: 0.0087 - val_loss: 5.2983 - val_acc: 0.0051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "eCkBV4Lr406B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion: "
      ]
    },
    {
      "metadata": {
        "id": "4ZAWMRBM406C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Experiment 3 is the best. we can add more fully connected layers and dropouts to improve the accuracy more"
      ]
    }
  ]
}
